{
    "collab_server" : "",
    "contents" : "library(tidyverse)\nlibrary(reshape2)\nlibrary(caret)\nlibrary(ROCR)\nlibrary(FFTrees)\nlibrary(boot)\n\ndata <- read.csv(\"data/final_data.csv\")\n\nset.seed(40) # Reproducibility\n\ndata$Group <- as.factor(data$Group)\n\n# Cross-validation https://www.youtube.com/watch?v=p5rDg4OVBuA. Need to create validation set \nindices <- createDataPartition(data$Group, p = 0.7, list = FALSE) #70% of data will be for training\ntrain <- data[indices,]\ntest <- data[-indices,]\n\n# Shuffling data:\n\ntrain <- train[sample(nrow(train)),]\n# After cross validation, testing on training data. Then get best model --> predict for test data\n\ncontrolParameters <- trainControl(\n  method = \"LOOCV\", #Leave one out cross validation\n  savePrediction = TRUE,\n  classProbs = TRUE\n)\n\nparameterGrid <- data.frame(mtry = c(2,3,4)) # Parameter for optimization: https://stats.stackexchange.com/questions/102867/random-forest-mtry-question\n\naccuracyMetric <- function (conMatrix) {\n  tp <- conMatrix[1,1]\n  fp <- conMatrix[1,2]\n  fn <- conMatrix[2,1]\n  tn <- conMatrix[2,2]\n  \n  metric <- (tp + tn)/(tp + fp + fn + tn)\n  return(metric)\n}\n\nmccMetric <- function (conMatrix) {\n  tp <- conMatrix[1,1]\n  fp <- conMatrix[1,2]\n  fn <- conMatrix[2,1]\n  tn <- conMatrix[2,2]\n  \n  metric <- ((tp * tn) - (fp * fn))/(sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))\n  return(metric)\n}\n\n# Training random forest model\nrfmodel <- train(Group ~.,\n  data = train,\n  method = \"rf\",\n  trControl = controlParameters,\n  tuneGrid = parameterGrid\n)\n\n\nplot(varImp(rfmodel), main = \"Variable Importance - Random Forest\")\n\n\nrfPredictions <- predict(rfmodel, test, type = \"prob\")\nconfMatrixrf <- table(predictions = rfPredictions, actual = test$Group)\nrfAccuracy <- accuracyMetric(confMatrixrf)\nrfMCC <- mccMetric(confMatrixrf)\n\n#Model: loadRDS(\"random_forest.rds\") OR loadRDS(\"loocv_rf.rds\")\n\n# TUNING PARAMETERS: https://www.analyticsvidhya.com/blog/2016/08/practicing-machine-learning-techniques-in-r-with-mlr-package/\n\n# Training logistic regression model. No tuning parameters?\n\nlogmodel <- train(Group ~ .,\n  data = train,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = controlParameters)\n\nlogmodel <- readRDS(\"log_model.rds\")\n\nplot(varImp(logmodel), main = \"Variable Importance - Logistic Regression\")\n\nlogPredictions <- predict(logmodel, test, type = \"prob\")\nconfMatrixlog <- table(predictions = logPredictions, actual = test$Group)\nlogAccuracy <- accuracyMetric(confMatrixlog)\nlogMCC <- mccMetric(confMatrixlog)\n# MAKE ROC curves\n\n# Training CART model\nCARTmodel <- train(Group ~.,\n  data = train,\n  method = \"rpart\",\n  trControl = controlParameters\n)\n\nCARTmodel <- readRDS(\"CARTmodel.rds\")\n\nrpart.plot::rpart.plot(CARTmodel$finalModel)\n\ncartPredictions <- predict(CARTmodel, test, type = \"prob\")\nconfMatrixCART <- table(predictions = cartPredictions, actual = test$Group)\ncartAccuracy <- accuracyMetric(confMatrixCART)\ncartMCC <- mccMetric(confMatrixCART)\n\n# Training FFT\nfft.train <- train\nfft.test <- test\nfft.train$target <- NULL\nfft.test$target <- NULL\n\nfft.train$target[fft.train$Group == \"AD\"] <- 1\nfft.test$target[fft.test$Group == \"AD\"] <- 1\nfft.train$target[fft.train$Group == \"CONTROL\"] <- 0\nfft.test$target[fft.test$Group == \"CONTROL\"] <- 0\n\nfft.train <- fft.train[,2:6]\nfft.test <- fft.test[,2:6]\n\nfftModel <- FFTrees(formula = target ~.,\n                    data = fft.train,\n                    data.test = fft.test)\n\nfftModel <- readRDS(\"fft_model.rds\")\n\nplot(fftModel, main = \"Fast and Frugal Tree\", decision.labels = c(\"AD\", \"Control\"))\n\n# Training KNN model\nparameterGrid <- expand.grid(k = c(2,3,4,5,6,7,8,9,10))\n\nknnModel <- train(Group ~ .,\n  data = train,\n  method = \"knn\",\n  trControl = controlParameters,\n  tuneGrid = parameterGrid)\nknnModel <- readRDS(\"knn_model.RDS\")\n\nknnPredictions <- predict(knnModel, test, type = \"prob\")\nconfMatrixKNN <- table(predictions = knnPredictions, actual = test$Group)\nknnAccuracy <- accuracyMetric(confMatrixKNN)\nknnMCC <- mccMetric(confMatrixKNN)\n\n# Training SVM model (radial kernel??)\nparameterGrid <- expand.grid(C = c(0.1,0.2,0.3,0.4,0.5,0.6,0.6,0.7,0.8,0.9,1), sigma = 0.5)\n\nsvmModel <- train (Group ~ .,\n  data = train,\n  method = \"svmRadial\",\n  trControl = controlParameters,\n  tuneGrid = parameterGrid)\nsvmModel <- readRDS(\"svm_model.RDS\")\n\nsvmPredictions <- predict(svmModel, test, type = \"prob\")\nconfMatrixSVM <- table(predictions = svmPredictions, actual = test$Group)\nsvmAccuracy <- accuracyMetric(confMatrixSVM)\nsvmMCC <- mccMetric(confMatrixSVM)\n\n#Stacking models: https://www.youtube.com/watch?v=k7sTiTWWCXM&t=228s\n\n## Correlation between models; fix this\n#model_corr <- list(mod1 = CARTmodel, mod2 = fftModel, mod3 = knnModel, mod4 = logmodel, mod5 = rfmodel, mod6 = svmModel)\n#modelCor(model_corr)\n\ncartPredictions <- predict(CARTmodel, train)\nknnPredictions <- predict(knnModel, train)\nlogPredictions <- predict(logmodel, train)\nrfPredictions <- predict(rfmodel, train)\nsvmPredictions <- predict(svmModel, train)\n\ntest$cartPredictions <- predict(CARTmodel, test)\ntest$knnPredictions <- predict(knnModel, test)\ntest$logPredictions <- predict(logmodel, test)\ntest$rfPredictions <- predict(rfmodel, test)\ntest$svmPredictions <- predict(svmModel, test)\n\npredDF <- data.frame(cartPredictions, knnPredictions, logPredictions, rfPredictions, svmPredictions, class = train$Group)\n\ncombineModel <- train(as.factor(class) ~., method = \"rf\", data = predDF, trControl = controlParameters)\n\nensemblePredct <- predict(combineModel, test)\nconfMatrixEnsemble <- table(predictions = ensemblePredct, actual = test$Group)\nensembleAccuracy <- accuracyMetric(confMatrixEnsemble)\nensembleMCC <- mccMetric(confMatrixEnsemble)\n\n# Confidence intervals for random forest classifier\ncontrolParameters <- trainControl(\n  method = \"boot\",\n  number = 1000\n)\n\nparameterGrid <- expand.grid(k = c(2,3,4,5,6,7,8,9,10))\n\nknnModel <- train(Group ~.,\n                  data = train,\n                  method = \"knn\",\n                  trControl = controlParameters,\n                  tuneGrid = parameterGrid\n)\n\npredictions <- predict(knnModel, test)\ncmatrix <- table(predictions = predictions, actual = test$Group)\naccMetric <- accuracyMetric(cmatrix)\nmccMet <- mccMetric (cmatrix)\n\n# ROC \nplot(roc(test$Group, rfPredictions[,2]))\nplot(roc(test$Group, logPredictions[,2]))\nplot(roc(test$Group, svmPredictions[,2]))\nplot(roc(test$Group, knnPredictions[,2]))\nplot(roc(test$Group, cartPredictions[,2]))\n",
    "created" : 1522696561721.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1722743071",
    "id" : "1ABE0FDD",
    "lastKnownWriteTime" : 1522209218,
    "last_content_update" : 1522209218,
    "path" : "~/Documents/Science Fair /Grade 11/Data Analysis/Data Analysis/new_ml.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}